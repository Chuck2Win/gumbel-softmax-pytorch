{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab5c00e4-6f06-47c4-9892-dacdab04c34f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Code to implement VAE-gumple_softmax in pytorch\n",
    "# author: Devinder Kumar (devinder.kumar@uwaterloo.ca), modified by Yongfei Yan\n",
    "# The code has been modified from pytorch example vae code and inspired by the origianl \\\n",
    "# tensorflow implementation of gumble-softmax by Eric Jang.\n",
    "\n",
    "import argparse\n",
    "import numpy as np\n",
    "\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from torch import nn, optim\n",
    "from torch.nn import functional as F\n",
    "from torchvision import datasets, transforms\n",
    "from torchvision.utils import save_image\n",
    "\n",
    "parser = argparse.ArgumentParser(description='VAE MNIST Example')\n",
    "parser.add_argument('--batch-size', type=int, default=100, metavar='N',\n",
    "                    help='input batch size for training (default: 100)')\n",
    "parser.add_argument('--epochs', type=int, default=10, metavar='N',\n",
    "                    help='number of epochs to train (default: 10)')\n",
    "parser.add_argument('--temp', type=float, default=1.0, metavar='S',\n",
    "                    help='tau(temperature) (default: 1.0)')\n",
    "parser.add_argument('--no-cuda', action='store_true', default=False,\n",
    "                    help='enables CUDA training')\n",
    "parser.add_argument('--seed', type=int, default=1, metavar='S',\n",
    "                    help='random seed (default: 1)')\n",
    "parser.add_argument('--log-interval', type=int, default=10, metavar='N',\n",
    "                    help='how many batches to wait before logging training status')\n",
    "parser.add_argument('--hard', action='store_true', default=False,\n",
    "                    help='hard Gumbel softmax')\n",
    "\n",
    "args = parser.parse_args()\n",
    "args.cuda = not args.no_cuda and torch.cuda.is_available()\n",
    "\n",
    "torch.manual_seed(args.seed)\n",
    "if args.cuda:\n",
    "    torch.cuda.manual_seed(args.seed)\n",
    "\n",
    "kwargs = {'num_workers': 1, 'pin_memory': True} if args.cuda else {}\n",
    "train_loader = torch.utils.data.DataLoader(\n",
    "    datasets.MNIST('./data/MNIST', train=True, download=True,\n",
    "                   transform=transforms.ToTensor()),\n",
    "    batch_size=args.batch_size, shuffle=True, **kwargs)\n",
    "test_loader = torch.utils.data.DataLoader(\n",
    "    datasets.MNIST('./data/MNIST', train=False, transform=transforms.ToTensor()),\n",
    "    batch_size=args.batch_size, shuffle=True, **kwargs)\n",
    "\n",
    "\n",
    "def sample_gumbel(shape, eps=1e-20):\n",
    "    U = torch.rand(shape)\n",
    "    if args.cuda:\n",
    "        U = U.cuda()\n",
    "    return -torch.log(-torch.log(U + eps) + eps)\n",
    "\n",
    "\n",
    "def gumbel_softmax_sample(logits, temperature):\n",
    "    y = logits + sample_gumbel(logits.size())\n",
    "    return F.softmax(y / temperature, dim=-1)\n",
    "\n",
    "\n",
    "def gumbel_softmax(logits, temperature, hard=False):\n",
    "    \"\"\"\n",
    "    ST-gumple-softmax\n",
    "    input: [*, n_class]\n",
    "    return: flatten --> [*, n_class] an one-hot vector\n",
    "    \"\"\"\n",
    "    y = gumbel_softmax_sample(logits, temperature)\n",
    "    \n",
    "    if not hard:\n",
    "        return y.view(-1, latent_dim * categorical_dim)\n",
    "\n",
    "    shape = y.size()\n",
    "    _, ind = y.max(dim=-1)\n",
    "    y_hard = torch.zeros_like(y).view(-1, shape[-1])\n",
    "    y_hard.scatter_(1, ind.view(-1, 1), 1)\n",
    "    y_hard = y_hard.view(*shape)\n",
    "    # Set gradients w.r.t. y_hard gradients w.r.t. y\n",
    "    y_hard = (y_hard - y).detach() + y\n",
    "    return y_hard.view(-1, latent_dim * categorical_dim)\n",
    "\n",
    "\n",
    "class VAE_gumbel(nn.Module):\n",
    "    def __init__(self, temp):\n",
    "        super(VAE_gumbel, self).__init__()\n",
    "\n",
    "        self.fc1 = nn.Linear(784, 512)\n",
    "        self.fc2 = nn.Linear(512, 256)\n",
    "        self.fc3 = nn.Linear(256, latent_dim * categorical_dim)\n",
    "\n",
    "        self.fc4 = nn.Linear(latent_dim * categorical_dim, 256)\n",
    "        self.fc5 = nn.Linear(256, 512)\n",
    "        self.fc6 = nn.Linear(512, 784)\n",
    "\n",
    "        self.relu = nn.ReLU()\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "\n",
    "    def encode(self, x):\n",
    "        h1 = self.relu(self.fc1(x))\n",
    "        h2 = self.relu(self.fc2(h1))\n",
    "        return self.relu(self.fc3(h2))\n",
    "\n",
    "    def decode(self, z):\n",
    "        h4 = self.relu(self.fc4(z))\n",
    "        h5 = self.relu(self.fc5(h4))\n",
    "        return self.sigmoid(self.fc6(h5))\n",
    "\n",
    "    def forward(self, x, temp, hard):\n",
    "        q = self.encode(x.view(-1, 784))\n",
    "        q_y = q.view(q.size(0), latent_dim, categorical_dim)\n",
    "        z = gumbel_softmax(q_y, temp, hard)\n",
    "        return self.decode(z), F.softmax(q_y, dim=-1).reshape(*q.size())\n",
    "\n",
    "\n",
    "latent_dim = 30\n",
    "categorical_dim = 10  # one-of-K vector\n",
    "\n",
    "temp_min = 0.5\n",
    "ANNEAL_RATE = 0.00003\n",
    "\n",
    "model = VAE_gumbel(args.temp)\n",
    "if args.cuda:\n",
    "    model.cuda()\n",
    "optimizer = optim.Adam(model.parameters(), lr=1e-3)\n",
    "\n",
    "\n",
    "# Reconstruction + KL divergence losses summed over all elements and batch\n",
    "def loss_function(recon_x, x, qy):\n",
    "    BCE = F.binary_cross_entropy(recon_x, x.view(-1, 784), size_average=False) / x.shape[0]\n",
    "\n",
    "    log_ratio = torch.log(qy * categorical_dim + 1e-20)\n",
    "    KLD = torch.sum(qy * log_ratio, dim=-1).mean()\n",
    "\n",
    "    return BCE + KLD\n",
    "\n",
    "\n",
    "def train(epoch):\n",
    "    model.train()\n",
    "    train_loss = 0\n",
    "    temp = args.temp\n",
    "    for batch_idx, (data, _) in enumerate(train_loader):\n",
    "        if args.cuda:\n",
    "            data = data.cuda()\n",
    "        optimizer.zero_grad()\n",
    "        recon_batch, qy = model(data, temp, args.hard)\n",
    "        loss = loss_function(recon_batch, data, qy)\n",
    "        loss.backward()\n",
    "        train_loss += loss.item() * len(data)\n",
    "        optimizer.step()\n",
    "        if batch_idx % 100 == 1:\n",
    "            temp = np.maximum(temp * np.exp(-ANNEAL_RATE * batch_idx), temp_min)\n",
    "\n",
    "        if batch_idx % args.log_interval == 0:\n",
    "            print('Train Epoch: {} [{}/{} ({:.0f}%)]\\tLoss: {:.6f}'.format(\n",
    "                epoch, batch_idx * len(data), len(train_loader.dataset),\n",
    "                       100. * batch_idx / len(train_loader),\n",
    "                       loss.item()))\n",
    "\n",
    "    print('====> Epoch: {} Average loss: {:.4f}'.format(\n",
    "        epoch, train_loss / len(train_loader.dataset)))\n",
    "\n",
    "\n",
    "def test(epoch):\n",
    "    model.eval()\n",
    "    test_loss = 0\n",
    "    temp = args.temp\n",
    "    for i, (data, _) in enumerate(test_loader):\n",
    "        if args.cuda:\n",
    "            data = data.cuda()\n",
    "        recon_batch, qy = model(data, temp, args.hard)\n",
    "        test_loss += loss_function(recon_batch, data, qy).item() * len(data)\n",
    "        if i % 100 == 1:\n",
    "            temp = np.maximum(temp * np.exp(-ANNEAL_RATE * i), temp_min)\n",
    "        if i == 0:\n",
    "            n = min(data.size(0), 8)\n",
    "            comparison = torch.cat([data[:n],\n",
    "                                    recon_batch.view(args.batch_size, 1, 28, 28)[:n]])\n",
    "            save_image(comparison.data.cpu(),\n",
    "                       'data/reconstruction_' + str(epoch) + '.png', nrow=n)\n",
    "\n",
    "    test_loss /= len(test_loader.dataset)\n",
    "    print('====> Test set loss: {:.4f}'.format(test_loss))\n",
    "\n",
    "\n",
    "def run():\n",
    "    for epoch in range(1, args.epochs + 1):\n",
    "        train(epoch)\n",
    "        test(epoch)\n",
    "\n",
    "        M = 64 * latent_dim\n",
    "        np_y = np.zeros((M, categorical_dim), dtype=np.float32)\n",
    "        np_y[range(M), np.random.choice(categorical_dim, M)] = 1\n",
    "        np_y = np.reshape(np_y, [M // latent_dim, latent_dim, categorical_dim])\n",
    "        sample = torch.from_numpy(np_y).view(M // latent_dim, latent_dim * categorical_dim)\n",
    "        if args.cuda:\n",
    "            sample = sample.cuda()\n",
    "        sample = model.decode(sample).cpu()\n",
    "        save_image(sample.data.view(M // latent_dim, 1, 28, 28),\n",
    "                   'data/sample_' + str(epoch) + '.png')\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    run()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
